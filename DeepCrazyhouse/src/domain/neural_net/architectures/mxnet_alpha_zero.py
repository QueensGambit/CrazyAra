"""
@file: mxnet_alpha_zero.py
Created on 02.09.18
@project: jc39qevo-deep-learning-project
@author: queensgambit

Description of the alpha zero architecture for mxnet:

Mastering the game of Go without
human knowledge, Silver et al.

'The input features st are processed by a residual tower that consists of a single
convolutional block followed by either 19 or 39 residual blocks.
The convolutional block applies the following modules:
(1) A convolution of 256 filters of kernel size 3 ×​3 with stride 1
(2) Batch normalization 18
(3) A rectifier nonlinearity
Each residual block applies the following modules sequentially to its input:
(1) A convolution of 256 filters of kernel size 3 ×​3 with stride 1
(2) Batch normalization
(3) A rectifier nonlinearity
(4) A convolution of 256 filters of kernel size 3 ×​3 with stride 1
(5) Batch normalization
(6) A skip connection that adds the input to the block
(7) A rectifier nonlinearity
The output of the residual tower is passed into two separate ‘heads’ for
computing the policy and value. The policy head applies the following modules:
(1) A convolution of 2 filters of kernel size 1 ×​1 with stride 1
(2) Batch normalization
(3) A rectifier nonlinearity
(4) A fully connected linear layer that outputs a vector of size 192 +​ 1 =​ 362,
corresponding to logit probabilities for all intersections and the pass move
The value head applies the following modules:
(1) A convolution of 1 filter of kernel size 1 ×​1 with stride 1
(2) Batch normalization
(3) A rectifier nonlinearity
(4) A fully connected linear layer to a hidden layer of size 256
(5) A rectifier nonlinearity
(6) A fully connected linear layer to a scalar
(7) A tanh nonlinearity outputting a scalar in the range [−​1, 1]
The overall network depth, in the 20- or 40-block network, is 39 or 79 param­
eterized layers, respectively, for the residual tower, plus an additional 2 layers for
the policy head and 3 layers for the value head.
We note that a different variant of residual networks was simultaneously applied
to computer Go33 and achieved an amateur dan-level performance; however, this
was restricted to a single-headed policy network trained solely by supervised
learning.

Neural network architecture comparison. Figure 4 shows the results of a com­
parison between network architectures. Specifically, we compared four different
neural networks:
(1) dual–res: the network contains a 20-block residual tower, as described above,
followed by both a policy head and a value head. This is the architecture used in
AlphaGo Zero.
(2) sep–res: the network contains two 20-block residual towers. The first tower
is followed by a policy head and the second tower is followed by a value head.
(3) dual–conv: the network contains a non-residual tower of 12 convolutional
blocks, followed by both a policy head and a value head.
(4) sep–conv: the network contains two non-residual towers of 12 convolutional
blocks. The first tower is followed by a policy head and the second tower is followed
by a value head. This is the architecture used in AlphaGo Lee.
Each network was trained on a fixed dataset containing the final 2 million
games of self-play data generated by a previous run of AlphaGo Zero, using
stochastic gradient descent with the annealing rate, momentum and regulariza­
tion hyperparameters described for the supervised learning experiment; however,
cross-entropy and MSE components were weighted equally, since more data was
available.'
"""
import mxnet as mx
from mxnet.gluon import SymbolBlock


def residual_block(data, num_filter, name, bn_mom=0.9, workspace=256, act_type='relu'):
    """
    Returns a residual block without any max pooling operation
    :param data: Input data
    :param num_filter: Number of filters for all CNN-layers
    :param name: Name for the residual block
    :param bn_mom: Batch normalization momentum
    :param workspace: Workspace parameter for the conv operation
    :param act_type: Activation function to use
    :return:
    """

    conv1 = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=(3,3), pad=(1,1),
                               no_bias=True, workspace=workspace, name=name + '_conv1')
    bn1 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn1')
    act1 = mx.sym.Activation(data=bn1, act_type=act_type, name=name + '_act1')

    conv2 = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(3,3), stride=(1,1), pad=(1,1),
                               no_bias=True, workspace=workspace, name=name + '_conv2')
    bn2 = mx.sym.BatchNorm(data=conv2, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn2')

    shortcut = data
    sum = mx.sym.broadcast_add(bn2, shortcut, name=name+'_add')

    return mx.sym.Activation(data=sum, act_type=act_type, name=name + '_act')


def alpha_zero_symbol(num_filter=256, channels_value_head=1, channels_policy_head=81, workspace=2048, value_fc_size=256,
                      num_res_blocks=19, bn_mom=0.9, act_type='relu', n_labels=4992, grad_scale_value=0.01,
                      grad_scale_policy=0.99, select_policy_from_plane=False):
    """
    Creates the alpha zero model symbol based on the given parameters.

    :param num_filter: Used for all convolution operations. (Except the last 2)
    :param workspace: Parameter for convolution
    :param value_fc_size: Fully Connected layer size. Used for the value output
    :param num_res_blocks: Number of residual blocks to stack. In the paper they used 19 or 39 residual blocks
    :param bn_mom: batch normalization momentum
    :param act_type: Activation function which will be used for all intermediate layers
    :param n_labels: Number of labels the for the policy
    :param grad_scale_value: Constant scalar which the gradient for the value outputs are being scaled width.
                            (They used 1.0 for default and 0.01 in the supervised setting)
    :param grad_scale_policy: Constant scalar which the gradient for the policy outputs are being scaled width.
                            (They used 1.0 for default and 0.99 in the supervised setting)
    :return: mxnet symbol of the model
    """
    # get the input data
    data = mx.sym.Variable(name='data')

    # first initial convolution layer followed by batchnormalization
    body = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=(3, 3), pad=(1, 1),
                              no_bias=True, name="stem_conv0", workspace=workspace)
    body = mx.sym.BatchNorm(data=body, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='stem_bn0')
    body = mx.sym.Activation(data=body, act_type=act_type, name='stem_act0')

    # build residual tower
    for i in range(num_res_blocks):
        body = residual_block(body, num_filter, name='block%d' % i,
                              bn_mom=bn_mom, workspace=workspace)
    # for policy output
    if select_policy_from_plane:
        policy_out = mx.sym.Convolution(data=body, num_filter=channels_policy_head, kernel=(3, 3), pad=(1, 1),
                                        no_bias=True, name="policy_conv0", workspace=workspace)
        policy_out = mx.sym.BatchNorm(data=policy_out, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='policy_bn0')
        policy_out = mx.sym.Activation(data=policy_out, act_type=act_type, name='policy_act0')
        policy_out = mx.sym.Flatten(data=policy_out, name='policy_out')
        policy_out = mx.sym.SoftmaxOutput(data=policy_out, name='policy', grad_scale=grad_scale_policy)
    else:
        policy_out = mx.sym.Convolution(data=body, num_filter=channels_policy_head, kernel=(1, 1), pad=(0, 0),
                                        no_bias=True, name="policy_conv0", workspace=workspace)
        policy_out = mx.sym.BatchNorm(data=policy_out, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='policy_bn0')
        policy_out = mx.sym.Activation(data=policy_out, act_type=act_type, name='policy_act0')
        policy_out = mx.sym.Flatten(data=policy_out, name='policy_flatten0')
        policy_out = mx.sym.FullyConnected(data=policy_out, num_hidden=n_labels, name='policy_out')
        policy_out = mx.sym.SoftmaxOutput(data=policy_out, name='policy', grad_scale=grad_scale_policy)

    # for value output
    value_out = mx.sym.Convolution(data=body, num_filter=channels_value_head, kernel=(1, 1), pad=(0, 0),
                                   no_bias=True, name="value_conv0", workspace=workspace)
    value_out = mx.sym.BatchNorm(data=value_out, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='value_bn0')
    value_out = mx.sym.Activation(data=value_out, act_type=act_type, name='value_act0')
    value_out = mx.sym.Flatten(data=value_out, name='value_flatten0')
    value_out = mx.sym.FullyConnected(data=value_out, num_hidden=value_fc_size, name='value_fc0')
    value_out = mx.sym.Activation(data=value_out, act_type=act_type, name='value_act1')
    value_out = mx.sym.FullyConnected(data=value_out, num_hidden=1, name='value_fc1')
    value_out = mx.sym.Activation(data=value_out, act_type='tanh', name='value_out')
    value_out = mx.sym.LinearRegressionOutput(data=value_out, name='value', grad_scale=grad_scale_value)

    # group value_out and policy_out together
    sym = mx.symbol.Group([value_out, policy_out])

    return sym


def alpha_zero_resnet(n_labels=4992, channels=256,  channels_value_head=1, channels_policy_head=81,
                      num_res_blocks=19, value_fc_size=256,  bn_mom=0.9, act_type='relu'):
    sym = alpha_zero_symbol(num_filter=channels, channels_value_head=channels_value_head,
                            channels_policy_head=channels_policy_head, workspace=1024, value_fc_size=value_fc_size,
                            num_res_blocks=num_res_blocks, bn_mom=bn_mom, n_labels=n_labels, grad_scale_policy=1.,
                            grad_scale_value=1.,)
    """
    Constructs the alpha zero network for gluon representation using a symbol block
    """
    net = SymbolBlock(outputs=sym, inputs=mx.sym.var('data'))
    return net
