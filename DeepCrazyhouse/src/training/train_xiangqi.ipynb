{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "\n",
    "from DeepCrazyhouse.configs.main_config import main_config\n",
    "from DeepCrazyhouse.configs.train_config import TrainConfig, TrainObjects\n",
    "from DeepCrazyhouse.src.runtime.color_logger import enable_color_logging\n",
    "from DeepCrazyhouse.src.domain.variants.constants import NB_POLICY_MAP_CHANNELS, NB_LABELS\n",
    "from DeepCrazyhouse.src.domain.variants.plane_policy_representation import FLAT_PLANE_IDX\n",
    "\n",
    "from DeepCrazyhouse.src.preprocessing.dataset_loader import load_xiangqi_dataset\n",
    "\n",
    "from DeepCrazyhouse.src.training.lr_schedules.lr_schedules import *\n",
    "\n",
    "from DeepCrazyhouse.src.domain.neural_net.architectures.rise_mobile_v2 import rise_mobile_v2_symbol\n",
    "from DeepCrazyhouse.src.domain.neural_net.architectures.rise_mobile_v3 import rise_mobile_v3_symbol\n",
    "\n",
    "from DeepCrazyhouse.src.training.trainer_agent import TrainerAgent, evaluate_metrics, acc_sign, reset_metrics\n",
    "from DeepCrazyhouse.src.training.trainer_agent_mxnet import TrainerAgentMXNET, get_context\n",
    "\n",
    "enable_color_logging()\n",
    "\n",
    "print(\"mxnet version: \", mx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-athletics",
   "metadata": {},
   "source": [
    "# Main Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in main_config.keys():\n",
    "    print(key, \"= \", main_config[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-british",
   "metadata": {},
   "source": [
    "# Settings for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TrainConfig()\n",
    "to = TrainObjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the context to GPU is strongly recommended\n",
    "tc.context = \"gpu\" # Be sure to check the used devices!!!\n",
    "tc.device_id = 0\n",
    "\n",
    "# Used for reproducibility\n",
    "tc.seed = 7\n",
    "\n",
    "tc.export_weights = True\n",
    "tc.log_metrics_to_tensorboard = True\n",
    "tc.export_grad_histograms = True\n",
    "\n",
    "# div factor is a constant which can be used to reduce the batch size and learning rate respectively\n",
    "# use a value larger 1 if you enconter memory allocation errors\n",
    "tc.div_factor = 2\n",
    "\n",
    "# defines how often a new checkpoint will be saved and the metrics evaluated\n",
    "# (batch_steps = 1000 means that every 1000 batches the validation set gets processed)\n",
    "tc.batch_steps = 100 * tc.div_factor\n",
    "# k_steps_initial defines how many steps have been trained before\n",
    "# (k_steps_initial != 0 if you continue training from a checkpoint)\n",
    "tc.k_steps_initial = 0\n",
    "\n",
    "# these are the weights to continue training with\n",
    "tc.symbol_file = None # 'model-0.81901-0.713-symbol.json'\n",
    "tc.params_file = None #'model-0.81901-0.713-0498.params'\n",
    "\n",
    "#typically if you half the batch_size, you should double the lr\n",
    "tc.batch_size = int(1024 / tc.div_factor)\n",
    "\n",
    "# optimization parameters\n",
    "tc.optimizer_name = \"nag\"\n",
    "tc.max_lr = 0.35 / tc.div_factor\n",
    "tc.min_lr = 0.00001\n",
    "tc.max_momentum = 0.95\n",
    "tc.min_momentum = 0.8\n",
    "# loads a previous checkpoint if the loss increased significanly\n",
    "tc.use_spike_recovery = True\n",
    "# stop training as soon as max_spikes has been reached\n",
    "tc.max_spikes = 20\n",
    "# define spike threshold when the detection will be triggered\n",
    "tc.spike_thresh = 1.5\n",
    "# weight decay\n",
    "tc.wd = 1e-4\n",
    "tc.dropout_rate = 0 #0.15\n",
    "# weight the value loss a lot lower than the policy loss in order to prevent overfitting\n",
    "tc.val_loss_factor = 0.01\n",
    "tc.policy_loss_factor = 0.99\n",
    "tc.discount = 1.0\n",
    "\n",
    "tc.normalize = True\n",
    "tc.nb_epochs = 7\n",
    "# Boolean if potential legal moves will be selected from final policy output\n",
    "tc.select_policy_from_plane = True \n",
    "\n",
    "# define if policy training target is one-hot encoded a distribution (e.g. mcts samples, knowledge distillation)\n",
    "tc.sparse_policy_label = True\n",
    "# define if the policy data is also defined in \"select_policy_from_plane\" representation\n",
    "tc.is_policy_from_plane_data = False\n",
    "\n",
    "# Decide between mxnet and gluon style for training\n",
    "tc.use_mxnet_style = True \n",
    "\n",
    "# additional custom validation set files which will be logged to tensorboard\n",
    "tc.variant_metrics = None #[\"chess960\", \"koth\", \"three_check\"]\n",
    "\n",
    "tc.name_initials = \"Your Initials\"\n",
    "\n",
    "# enable data set augmentation\n",
    "augment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = main_config[\"mode\"]\n",
    "ctx = get_context(tc.context, tc.device_id)\n",
    "\n",
    "# if use_extra_variant_input is true the current active variant is passed two each residual block and\n",
    "# concatenated at the end of the final feature representation\n",
    "use_extra_variant_input = False\n",
    "\n",
    "# iteration counter used for the momentum and learning rate schedule\n",
    "cur_it = tc.k_steps_initial * tc.batch_steps \n",
    "\n",
    "# Fix the random seed\n",
    "mx.random.seed(tc.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-network",
   "metadata": {},
   "source": [
    "###  Crete a ./logs and ./weights directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./logs && mkdir ./weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-hygiene",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-firmware",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = True\n",
    "\n",
    "if combined:\n",
    "    start_indices_val_0, x_val_0, y_value_val_0, y_policy_val_0, dataset_0 = load_xiangqi_dataset(dataset_type=\"val\",\n",
    "                                                                                        part_id=0,\n",
    "                                                                                        verbose=True,\n",
    "                                                                                        normalize=tc.normalize)\n",
    "    start_indices_val_1, x_val_1, y_value_val_1, y_policy_val_1, dataset_1 = load_xiangqi_dataset(dataset_type=\"val\",\n",
    "                                                                                        part_id=1,\n",
    "                                                                                        verbose=True,\n",
    "                                                                                        normalize=tc.normalize)\n",
    "    # X\n",
    "    nb_inputs = x_val_0.shape[0] + x_val_1.shape[0]\n",
    "    nb_planes = x_val_0.shape[1]\n",
    "    nb_rows = x_val_0.shape[2]\n",
    "    nb_cols = x_val_0.shape[3]\n",
    "    x_val = np.zeros((nb_inputs, nb_planes, nb_rows, nb_cols))\n",
    "    x_val[:x_val_0.shape[0]] = x_val_0\n",
    "    x_val[x_val_0.shape[0]:] = x_val_1\n",
    "\n",
    "    # value targets\n",
    "    nb_targets_value = y_value_val_0.shape[0] + y_value_val_1.shape[0]\n",
    "    y_value_val = np.zeros((nb_targets_value,))\n",
    "    y_value_val[:y_value_val_0.shape[0]] = y_value_val_0\n",
    "    y_value_val[y_value_val_0.shape[0]:] = y_value_val_1\n",
    "\n",
    "    # policy targets\n",
    "    nb_targets_policy = y_policy_val_0.shape[0] + y_policy_val_1.shape[0]\n",
    "    y_policy_val = np.zeros((nb_targets_policy,y_policy_val_0.shape[1]))\n",
    "    y_policy_val[:y_policy_val_0.shape[0]] = y_policy_val_0\n",
    "    y_policy_val[y_policy_val_0.shape[0]:] = y_policy_val_1\n",
    "else:\n",
    "    start_indices_val, x_val, y_value_val, y_policy_val, dataset = load_xiangqi_dataset(dataset_type=\"val\",\n",
    "                                                                                        part_id=0,\n",
    "                                                                                        verbose=True,\n",
    "                                                                                        normalize=tc.normalize)\n",
    "if tc.normalize:\n",
    "    assert x_val.max() <= 1.0, \"Error: Normalization not working.\"\n",
    "\n",
    "if tc.select_policy_from_plane:\n",
    "    val_iter = mx.io.NDArrayIter({'data': x_val}, \n",
    "                                     {'value_label': y_value_val, \n",
    "                                      'policy_label': np.array(FLAT_PLANE_IDX)[y_policy_val.argmax(axis=1)]},\n",
    "                                     tc.batch_size)\n",
    "else:\n",
    "    val_iter = mx.io.NDArrayIter({'data': x_val}, \n",
    "                                     {'value_label': y_value_val, \n",
    "                                      'policy_label': y_policy_val.argmax(axis=1)}, \n",
    "                                     tc.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-correction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"x_val.shape: \", x_val.shape)\n",
    "print(\"y_value_val.shape: \", y_value_val.shape)\n",
    "print(\"y_policy_val.shape: \", y_policy_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-steal",
   "metadata": {},
   "source": [
    "### Training properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-occasion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tc.nb_parts = len(glob.glob(main_config[\"planes_train_dir\"] + \"**/*\"))\n",
    "print(\"Parts training dataset: \", tc.nb_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one iteration is defined by passing 1 batch and doing backpropagation\n",
    "if augment:\n",
    "    nb_it_per_epoch = (len(x_val) * tc.nb_parts * 2) // tc.batch_size\n",
    "else:\n",
    "    nb_it_per_epoch = (len(x_val) * tc.nb_parts) // tc.batch_size\n",
    "tc.total_it = int(nb_it_per_epoch * tc.nb_epochs)\n",
    "print(\"Total iterations: \", tc.total_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-tutorial",
   "metadata": {},
   "source": [
    "# Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "to.lr_schedule = OneCycleSchedule(start_lr=tc.max_lr/8, \n",
    "                               max_lr=tc.max_lr, \n",
    "                               cycle_length=tc.total_it*.3, \n",
    "                               cooldown_length=tc.total_it*.6, \n",
    "                               finish_lr=tc.min_lr)\n",
    "to.lr_schedule = LinearWarmUp(to.lr_schedule, start_lr=tc.min_lr, length=tc.total_it/30)\n",
    "plot_schedule(to.lr_schedule, iterations=tc.total_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-thumbnail",
   "metadata": {},
   "source": [
    "# Momentum Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-wyoming",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to.momentum_schedule = MomentumSchedule(to.lr_schedule, tc.min_lr, tc.max_lr, tc.min_momentum, tc.max_momentum)\n",
    "plot_schedule(to.momentum_schedule, iterations=tc.total_it, ylabel='Momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-advantage",
   "metadata": {},
   "source": [
    "# Define NN model / Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_val[0].shape\n",
    "print(\"Input shape: \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_res_blocks = [3] * 5 # 13\n",
    "if tc.symbol_file is None:\n",
    "    # channels_operating_init, channel_expansion\n",
    "    symbol = rise_mobile_v2_symbol(channels=256, channels_operating_init=512, channel_expansion=0, channels_value_head=8,\n",
    "                      channels_policy_head=NB_POLICY_MAP_CHANNELS, value_fc_size=256, bc_res_blocks=bc_res_blocks, res_blocks=[], act_type='relu',\n",
    "                      n_labels=NB_LABELS, grad_scale_value=tc.val_loss_factor, grad_scale_policy=tc.policy_loss_factor, select_policy_from_plane=tc.select_policy_from_plane,\n",
    "                      use_se=True, dropout_rate=tc.dropout_rate, use_extra_variant_input=use_extra_variant_input)\n",
    "else:\n",
    "    symbol = mx.sym.load(\"weights/\" + symbol_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bc_res_blocks = [3] * 13 \n",
    "if tc.symbol_file is None:\n",
    "    # channels_operating_init, channel_expansion\n",
    "    symbol = rise_mobile_v2_symbol(channels=256, channels_operating_init=128, channel_expansion=64, channels_value_head=8,\n",
    "                      channels_policy_head=NB_POLICY_MAP_CHANNELS, value_fc_size=256, bc_res_blocks=bc_res_blocks, res_blocks=[], act_type='relu',\n",
    "                      n_labels=NB_LABELS, grad_scale_value=tc.val_loss_factor, grad_scale_policy=tc.policy_loss_factor, select_policy_from_plane=tc.select_policy_from_plane,\n",
    "                      use_se=True, dropout_rate=tc.dropout_rate, use_extra_variant_input=use_extra_variant_input)\n",
    "else:\n",
    "    symbol = mx.sym.load(\"weights/\" + symbol_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-walter",
   "metadata": {},
   "source": [
    "# Network summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-springer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(mx.viz.plot_network(\n",
    "        symbol,\n",
    "        shape={'data':(1, input_shape[0], input_shape[1], input_shape[2])},\n",
    "        node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.viz.print_summary(\n",
    "        symbol,\n",
    "        shape={'data':(1, input_shape[0], input_shape[1], input_shape[2])},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-cleveland",
   "metadata": {},
   "source": [
    "# Initialize weights if no pretrained weights are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a trainable module on compute context\n",
    "model = mx.mod.Module(symbol=symbol, context=ctx, label_names=['value_label', 'policy_label'])\n",
    "model.bind(for_training=True, data_shapes=[('data', (tc.batch_size, input_shape[0], input_shape[1], input_shape[2]))],\n",
    "          label_shapes=val_iter.provide_label)\n",
    "model.init_params(mx.initializer.Xavier(rnd_type='uniform', factor_type='avg', magnitude=2.24))\n",
    "if tc.params_file:\n",
    "    model.load_params(\"weights/\" + tc.params_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-barrier",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mxnet = [\n",
    "mx.metric.MSE(name='value_loss', output_names=['value_output'], label_names=['value_label']),\n",
    "mx.metric.CrossEntropy(name='policy_loss', output_names=['policy_output'],\n",
    "                                            label_names=['policy_label']),\n",
    "mx.metric.create(acc_sign, name='value_acc_sign', output_names=['value_output'],\n",
    "                                         label_names=['value_label']),\n",
    "mx.metric.Accuracy(axis=1, name='policy_acc', output_names=['policy_output'],\n",
    "                                       label_names=['policy_label'])\n",
    "]\n",
    "metrics_gluon = {\n",
    "'value_loss': mx.metric.MSE(name='value_loss', output_names=['value_output']),\n",
    "'policy_loss': mx.metric.CrossEntropy(name='policy_loss', output_names=['policy_output'],\n",
    "                                            label_names=['policy_label']),\n",
    "'value_acc_sign': mx.metric.create(acc_sign, name='value_acc_sign', output_names=['value_output'],\n",
    "                                         label_names=['value_label']),\n",
    "'policy_acc': mx.metric.Accuracy(axis=1, name='policy_acc', output_names=['policy_output'],\n",
    "                                       label_names=['policy_label'])\n",
    "}\n",
    "to.metrics = metrics_mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-poetry",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent = TrainerAgentMXNET(model, symbol, val_iter, tc, to, use_rtpt=False, augment=augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-sense",
   "metadata": {},
   "source": [
    "# Performance Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(val_iter, to.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-wrist",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-piece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(k_steps_final, value_loss_final, policy_loss_final, value_acc_sign_final, val_p_acc_final), (k_steps_best, val_loss_best, val_p_acc_best) = train_agent.train(cur_it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
