{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script for the CNN \n",
    "\n",
    "Loads in the converted plane representation of the pgn files, defines the network architecture and starts the training process. Checkpoints of the weights are saved if there's an improvement in the validation loss.\n",
    "The training performance metrics (e.g. losses, accuracies...) are exported to tensorboard and can be checked during training.\n",
    "* author: QueensGambit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'../../../')\n",
    "import glob\n",
    "import chess\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from DeepCrazyhouse.src.training.train_util import get_metrics, prepare_policy, value_to_wdl_label, prepare_plys_label\n",
    "from DeepCrazyhouse.src.domain.variants.input_representation import board_to_planes, planes_to_board\n",
    "from DeepCrazyhouse.src.domain.variants.output_representation import policy_to_moves, policy_to_best_move, policy_to_move\n",
    "from DeepCrazyhouse.src.preprocessing.dataset_loader import load_pgn_dataset\n",
    "from DeepCrazyhouse.src.runtime.color_logger import enable_color_logging\n",
    "from DeepCrazyhouse.configs.main_config import main_config\n",
    "from DeepCrazyhouse.configs.train_config import TrainConfig, TrainObjects\n",
    "\n",
    "from DeepCrazyhouse.src.training.lr_schedules.lr_schedules import *\n",
    "from DeepCrazyhouse.src.domain.variants.plane_policy_representation import FLAT_PLANE_IDX\n",
    "from DeepCrazyhouse.src.domain.variants.constants import NB_POLICY_MAP_CHANNELS, NB_LABELS, MODE_CHESS, MODE_CRAZYHOUSE\n",
    "enable_color_logging()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TrainConfig()\n",
    "to = TrainObjects()\n",
    "# Decide between 'pytorch', 'mxnet' and 'gluon' style for training\n",
    "tc.framework: str = 'pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'pytorch':\n",
    "    import torch\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    from torchsummary import summary\n",
    "    from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "    from DeepCrazyhouse.src.training.trainer_agent_pytorch import TrainerAgentPytorch, save_torch_state,\\\n",
    "    load_torch_state, export_to_onnx, get_context, get_data_loader, evaluate_metrics\n",
    "    # architectures\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.rise_mobile_v3 import RiseV3, get_rise_v33_model_by_train_config\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.vision_transformer import VisionTransformer\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.vit_configs import get_b8_config\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.le_vit import LeViT\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.mobile_vit import MobileViT\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.trt_vit import TrtViT\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.next_vit_official import NextVit\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.pytorch.a0_resnet import AlphaZeroResnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mxnet imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet' or tc.framework == 'gluon':\n",
    "    import mxnet as mx\n",
    "    from mxnet import nd\n",
    "    from mxnet import gluon\n",
    "    try:\n",
    "        import mxnet.metric as metric\n",
    "    except ModuleNotFoundError:\n",
    "        import mxnet.gluon.metric as metrics\n",
    "\n",
    "    from DeepCrazyhouse.src.training.trainer_agent_gluon import TrainerAgentGluon, evaluate_metrics, acc_sign, get_data_loader\n",
    "    from DeepCrazyhouse.src.training.trainer_agent_mxnet import TrainerAgentMXNET, get_context\n",
    "    # architectures\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.a0_resnet import AlphaZeroResnet\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.mxnet_alpha_zero import alpha_zero_symbol\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.rise_mobile_v2 import rise_mobile_v2_symbol\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.rise_mobile_v3 import rise_mobile_v3_symbol\n",
    "    from DeepCrazyhouse.src.domain.neural_net.architectures.preact_resnet_se import preact_resnet_se\n",
    "    from DeepCrazyhouse.src.domain.neural_net.onnx.convert_to_onnx import convert_mxnet_model_to_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the context on CPU, switch to GPU if there is one available (strongly recommended for training)\n",
    "tc.context = \"gpu\"\n",
    "tc.device_id = 0\n",
    "\n",
    "# set a specific seed value for reproducibility\n",
    "tc.seed = 9 # 42\n",
    "\n",
    "tc.export_weights = True\n",
    "tc.log_metrics_to_tensorboard = True\n",
    "tc.export_grad_histograms = False\n",
    "\n",
    "phase_weights = {0: 1.0, 1: 1.0, 2: 1.0}  # specify the sample weight for each phase (will be normalized afterwards)\n",
    "if \"movecount\" in main_config[\"phase_definition\"]:\n",
    "    assert len(phase_weights) == int(main_config[\"phase_definition\"][-1])\n",
    "\n",
    "# directory to write and read weights, logs, onnx and other export files\n",
    "#tc.export_dir = \"C:/workspace/Python/CrazyAra/data/train_phase2/\"\n",
    "tc.export_dir = f\"/data/run_model_exports_movecount/movecount4_train_phase_0/\"\n",
    "\n",
    "tc.div_factor = 0.5  # div factor is a constant which can be used to reduce the batch size and learning rate respectively\n",
    "# use a value greater 1 if you encounter memory allocation errors\n",
    "\n",
    "# batch_steps = 1000 means for example that every 1000 batches the validation set gets processed\n",
    "tc.batch_steps = 1000 * tc.div_factor # this defines how often a new checkpoint will be saved and the metrics evaluated\n",
    "# k_steps_initial defines how many steps have been trained before\n",
    "# (k_steps_initial != 0 if you continue training from a checkpoint)\n",
    "tc.k_steps_initial = 0\n",
    "# these are the weights to continue training with\n",
    "tc.symbol_file = None # 'model-0.81901-0.713-symbol.json'\n",
    "tc.tar_file = None # f\"/data/run_model_exports/train_phase_None_0_25_0_25_1_0/best-model/model-1.25307-0.567-0529.tar\" #'model-0.81901-0.713-0498.params'  # used to continue training from model params checkpoint\n",
    "\n",
    "tc.batch_size = int(1024 / tc.div_factor) # 1024 # the batch_size needed to be reduced to 1024 in order to fit in the GPU 1080Ti\n",
    "#4096 was originally used in the paper -> works slower for current GPU\n",
    "# 2048 was used in the paper Mastering the game of Go without human knowledge and fits in GPU memory\n",
    "#typically if you half the batch_size, you should double the lr\n",
    "\n",
    "# optimization parameters\n",
    "tc.optimizer_name = \"nag\"  # \"adam\" \"adamw\" # \n",
    "if tc.framework == 'pytorch':\n",
    "    # strangely pytorch should use a different lr than mxnet\n",
    "    tc.max_lr = 0.07 / tc.div_factor\n",
    "else:\n",
    "    tc.max_lr = 0.35 / tc.div_factor #0.01 # default lr for adam\n",
    "tc.min_lr = 0.00001\n",
    "\n",
    "if \"adam\" in tc.optimizer_name:\n",
    "    tc.max_lr = 0.001001 #1e-3\n",
    "    tc.min_lr = 0.001\n",
    "    \n",
    "tc.max_momentum = 0.95\n",
    "tc.min_momentum = 0.8\n",
    "# loads a previous checkpoint if the loss increased significanly\n",
    "tc.use_spike_recovery = True\n",
    "# stop training as soon as max_spikes has been reached\n",
    "tc.max_spikes = 20\n",
    "# define spike threshold when the detection will be triggered\n",
    "tc.spike_thresh = 1.5\n",
    "# weight decay\n",
    "tc.wd = 1e-4\n",
    "tc.dropout_rate = 0 #0.15\n",
    "\n",
    "# enables training with a wdl head as intermediate target (mainly useful for environments with 3 outcomes)\n",
    "tc.use_wdl = True\n",
    "# enables training with ply to end head\n",
    "tc.use_plys_to_end = True\n",
    "# adds a small mlp to infer the value loss from wdl and plys_to_end_output\n",
    "tc.use_mlp_wdl_ply = False\n",
    "\n",
    "# weight the value loss a lot lower than the policy loss in order to prevent overfitting\n",
    "tc.val_loss_factor = 0.01\n",
    "tc.policy_loss_factor = 0.988 if tc.use_plys_to_end else 0.99\n",
    "tc.plys_to_end_loss_factor = 0.002\n",
    "tc.wdl_loss_factor = 0.01\n",
    "tc.discount = 1.0\n",
    "\n",
    "tc.normalize = True # define whether to normalize input data to [0,1]\n",
    "tc.nb_training_epochs = 7 # define how many epochs the network will be trained\n",
    "tc.select_policy_from_plane = True # Boolean if potential legal moves will be selected from final policy output\n",
    "        \n",
    "# additional custom validation set files which will be logged to tensorboard\n",
    "to.variant_metrics = None # [\"chess960\", \"koth\", \"three_check\"]\n",
    "# if use_extra_variant_input is true the current active variant is passed two each residual block and\n",
    "\n",
    "# ratio for mixing the value return with the corresponding q-value\n",
    "# for a ratio of 0 no q-value information will be used\n",
    "tc.q_value_ratio = 0\n",
    "\n",
    "# define if policy training target is one-hot encoded a distribution (e.g. mcts samples, knowledge distillation)\n",
    "tc.sparse_policy_label = True\n",
    "# define if the policy data is also defined in \"select_policy_from_plane\" representation\n",
    "tc.is_policy_from_plane_data = False\n",
    "tc.name_initials = \"FH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_weights_sum = sum(phase_weights.values())\n",
    "to.phase_weights = {k: v/phase_weights_sum*len(phase_weights) for k, v in phase_weights.items()}  # normalize so that the average weight is 1.0 (assuming each phase occurs approximately equally often)\n",
    "mode = main_config[\"mode\"]\n",
    "ctx = get_context(tc.context, tc.device_id)\n",
    "# concatenated at the end of the final feature representation\n",
    "use_extra_variant_input = False\n",
    "cur_it = tc.k_steps_initial * tc.batch_steps # iteration counter used for the momentum and learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet' or tc.framework == 'gluon':\n",
    "    # Fixing the random seed\n",
    "    mx.random.seed(tc.seed)\n",
    "    mx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create logs and weights directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(tc.export_dir + \"logs\"):\n",
    "    os.mkdir(tc.export_dir + \"logs\")\n",
    "if not os.path.exists(tc.export_dir + \"weights\"):\n",
    "    os.mkdir(tc.export_dir + \"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataset (which is used during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn_dataset_arrays_dict = load_pgn_dataset(dataset_type='val', part_id=0,\n",
    "                                           verbose=True, normalize=tc.normalize)\n",
    "s_idcs_val = pgn_dataset_arrays_dict[\"start_indices\"]\n",
    "x_val = pgn_dataset_arrays_dict[\"x\"]\n",
    "yv_val = pgn_dataset_arrays_dict[\"y_value\"]\n",
    "yp_val = pgn_dataset_arrays_dict[\"y_policy\"]\n",
    "plys_to_end = pgn_dataset_arrays_dict[\"plys_to_end\"]\n",
    "pgn_datasets_val = pgn_dataset_arrays_dict[\"pgn_dataset\"]\n",
    "phase_vector = pgn_dataset_arrays_dict[\"phase_vector\"]\n",
    "\n",
    "if tc.discount != 1:\n",
    "    yv_val *= tc.discount**plys_to_end\n",
    "    \n",
    "if tc.framework == 'mxnet':\n",
    "    if tc.select_policy_from_plane:\n",
    "        if tc.use_wdl and tc.use_wdl:\n",
    "            val_iter = mx.io.NDArrayIter({'data': x_val},\n",
    "                                         {'value_label': yv_val,\n",
    "                                          'policy_label': np.array(FLAT_PLANE_IDX)[yp_val.argmax(axis=1)],\n",
    "                                          'wdl_label': value_to_wdl_label(yv_val),\n",
    "                                          'plys_to_end_label': prepare_plys_label(plys_to_end)},\n",
    "                                          tc.batch_size)\n",
    "        else:\n",
    "            val_iter = mx.io.NDArrayIter({'data': x_val},\n",
    "                                         {'value_label': yv_val,\n",
    "                                          'policy_label': np.array(FLAT_PLANE_IDX)[yp_val.argmax(axis=1)]},\n",
    "                                         tc.batch_size)\n",
    "    else:\n",
    "        val_iter = mx.io.NDArrayIter({'data': x_val}, {'value_label': yv_val, 'policy_label': yp_val.argmax(axis=1)}, tc.batch_size)\n",
    "elif tc.framework == 'gluon' or tc.framework == 'pytorch':\n",
    "    val_data = get_data_loader(pgn_dataset_arrays_dict, tc, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional eval sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill additional loaders that should be used for additional evaluations during training\n",
    "if tc.framework == 'pytorch':\n",
    "    additional_data_loaders = dict()\n",
    "    for phase in [str(phase) for phase in to.phase_weights.keys()] + [\"None\"]:\n",
    "        pgn_dataset_arrays_dict = load_pgn_dataset(dataset_type='test', part_id=0,\n",
    "                                                   verbose=True, normalize=tc.normalize, phase=phase)\n",
    "        data_loader = get_data_loader(pgn_dataset_arrays_dict, tc, shuffle=False)\n",
    "        additional_data_loaders[f\"Phase{phase}Test\"] = data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.nb_parts = len(glob.glob(main_config['planes_train_dir'] + '**/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_it_per_epoch = (len(x_val) * tc.nb_parts) // tc.batch_size # calculate how many iterations per epoch exist\n",
    "# one iteration is defined by passing 1 batch and doing backprop\n",
    "tc.total_it = int(nb_it_per_epoch * tc.nb_training_epochs)\n",
    "tc.total_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Learning Rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"adam\" in tc.optimizer_name:\n",
    "    to.lr_schedule = ConstantSchedule(tc.min_lr)\n",
    "else:\n",
    "    to.lr_schedule = OneCycleSchedule(start_lr=tc.max_lr/8, max_lr=tc.max_lr, cycle_length=tc.total_it*.3, cooldown_length=tc.total_it*.6, finish_lr=tc.min_lr)\n",
    "to.lr_schedule = LinearWarmUp(to.lr_schedule, start_lr=tc.min_lr, length=tc.total_it/30)\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "plot_schedule(to.lr_schedule, iterations=tc.total_it)\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.momentum_schedule = MomentumSchedule(to.lr_schedule, tc.min_lr, tc.max_lr, tc.min_momentum, tc.max_momentum)\n",
    "plot_schedule(to.momentum_schedule, iterations=tc.total_it, ylabel='Momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_val[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del net\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NN model / Load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MXNet model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = AlphaZeroResnet(n_labels=2272, channels=256, channels_value_head=8, channels_policy_head=81, num_res_blocks=19, value_fc_size=256, bn_mom=0.9, act_type='relu', select_policy_from_plane=select_policy_from_plane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = alpha_zero_resnet(n_labels=2272, channels=256, channels_value_head=1, channels_policy_head=81, num_res_blocks=19, value_fc_size=256, bn_mom=0.9, act_type='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbol = alpha_zero_symbol(num_filter=256, channels_value_head=4, channels_policy_head=81, workspace=1024, value_fc_size=256, num_res_blocks=19, bn_mom=0.9, act_type='relu',\n",
    "#                            n_labels=2272, grad_scale_value=0.01, grad_scale_policy=0.99, select_policy_from_plane=select_policy_from_plane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bc_res_blocks = [3] * 13\n",
    "#if tc.symbol_file is None:\n",
    "#    symbol = rise_mobile_v2_symbol(channels=256, channels_operating_init=128, channel_expansion=64, channels_value_head=8,\n",
    "#                      channels_policy_head=NB_POLICY_MAP_CHANNELS, value_fc_size=256, bc_res_blocks=bc_res_blocks, res_blocks=[], act_type='relu',\n",
    "#                      n_labels=NB_LABELS, grad_scale_value=tc.val_loss_factor, grad_scale_policy=tc.policy_loss_factor, select_policy_from_plane=tc.select_policy_from_plane,\n",
    "#                      use_se=True, dropout_rate=tc.dropout_rate, use_extra_variant_input=use_extra_variant_input)\n",
    "#else:\n",
    "#    symbol = mx.sym.load(tc.export_dir + \"weights/\" + tc.symbol_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernels = [3] * 15\n",
    "kernels[7] = 5\n",
    "kernels[11] = 5\n",
    "kernels[12] = 5\n",
    "kernels[13] = 5\n",
    "\n",
    "se_types = [None] * len(kernels)\n",
    "se_types[5] = \"eca_se\"\n",
    "se_types[8] = \"eca_se\"\n",
    "se_types[12] = \"eca_se\"\n",
    "se_types[13] = \"eca_se\"\n",
    "se_types[14] = \"eca_se\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernels = [3] * 7\n",
    "\n",
    "se_types = [None] * len(kernels)\n",
    "se_types[5] = \"eca_se\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "symbol = rise_mobile_v3_symbol(channels=256, channels_operating_init=224, channel_expansion=32, act_type='relu',\n",
    "                               channels_value_head=8, value_fc_size=256,\n",
    "                               channels_policy_head=NB_POLICY_MAP_CHANNELS,\n",
    "                               grad_scale_value=tc.val_loss_factor,\n",
    "                               grad_scale_policy=tc.policy_loss_factor,\n",
    "                               grad_scale_wdl=tc.wdl_loss_factor,\n",
    "                               grad_scale_ply=tc.plys_to_end_loss_factor,\n",
    "                               dropout_rate=tc.dropout_rate, select_policy_from_plane=True,\n",
    "                               kernels=kernels, se_types=se_types, use_avg_features=False,\n",
    "                               use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end,\n",
    "                               use_mlp_wdl_ply=tc.use_mlp_wdl_ply\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernels = [3,3,3,3,3,3,5,5]\n",
    "\n",
    "se_types = [\n",
    "    None, # 1\n",
    "    None, # 2\n",
    "    None,  # 3\n",
    "    \"eca_se\",  # 4\n",
    "    None, # 5\n",
    "    None,  # 6\n",
    "    None, # 7\n",
    "    \"eca_se\", # 8\n",
    "] \n",
    "\n",
    "symbol = preact_resnet_se(channels=288, act_type='relu',\n",
    "                          channels_value_head=8, value_fc_size=256,\n",
    "                          channels_policy_head=NB_POLICY_MAP_CHANNELS,\n",
    "                          grad_scale_value=tc.val_loss_factor, grad_scale_policy=tc.policy_loss_factor, \n",
    "                          dropout_rate=tc.dropout_rate, select_policy_from_plane=True,\n",
    "                          kernels=kernels, se_types=se_types, use_avg_features=True, use_raw_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_rise_v33_model_by_train_config(input_shape, tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = LeViT(\n",
    "    image_size = input_shape[1],\n",
    "    in_channels = input_shape[0],\n",
    "    channels_policy_head = NB_POLICY_MAP_CHANNELS,\n",
    "    stages = 1,             # number of stages\n",
    "    dim = (256,),  # dimensions at each stage\n",
    "    depth = 9,              # transformer of depth 4 at each stage\n",
    "    heads = (4,),      # heads at each stage\n",
    "    mlp_mult = 2,\n",
    "    dropout = 0.1,\n",
    "    select_policy_from_plane=tc.select_policy_from_plane,\n",
    "    use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end,\n",
    "    use_mlp_wdl_ply=tc.use_mlp_wdl_ply,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = MobileViT(\n",
    "    image_size = (input_shape[1], input_shape[2]),\n",
    "    in_channels = input_shape[0],\n",
    "    dims = [96, 120, 144],\n",
    "    channels = 256,\n",
    "    channels_policy_head = NB_POLICY_MAP_CHANNELS,\n",
    "    select_policy_from_plane=tc.select_policy_from_plane,\n",
    "    use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end,\n",
    "    use_mlp_wdl_ply=tc.use_mlp_wdl_ply,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = TrtViT(\n",
    "    image_size = input_shape[1],\n",
    "    in_channels = input_shape[0],\n",
    "    channels_policy_head = NB_POLICY_MAP_CHANNELS,\n",
    "    channels=256,\n",
    "    select_policy_from_plane=tc.select_policy_from_plane,\n",
    "    use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end,\n",
    "    use_mlp_wdl_ply=tc.use_mlp_wdl_ply,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernels = [3] * 12 #15\n",
    "kernels[10] = 5\n",
    "kernels[9] = 5\n",
    "kernels[5] = 5\n",
    "#kernels[13] = 5\n",
    "\n",
    "use_transformers = [False] * len(kernels)\n",
    "use_transformers[11] = True\n",
    "#use_transformers[9] = True\n",
    "#use_transformers[4] = True\n",
    "\n",
    "se_types = [None] * len(kernels)\n",
    "se_types[5] = \"eca_se\"\n",
    "se_types[11] = \"eca_se\"\n",
    "#se_types[12] = \"eca_se\"\n",
    "#se_types[13] = \"eca_se\"\n",
    "#se_types[14] = \"eca_se\"\n",
    "\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.input_shape = input_shape\n",
    "args.channels_policy_head = NB_POLICY_MAP_CHANNELS\n",
    "args.n_labels = NB_LABELS\n",
    "args.select_policy_from_plane = tc.select_policy_from_plane\n",
    "args.use_wdl = tc.use_wdl\n",
    "args.use_plys_to_end = tc.use_plys_to_end\n",
    "args.use_mlp_wdl_ply = tc.use_mlp_wdl_ply\n",
    "\n",
    "model = RiseV3(nb_input_channels=args.input_shape[0], board_height=args.input_shape[1], board_width=args.input_shape[2],\n",
    "                channels=256, channels_operating_init=224, channel_expansion=32,\n",
    "                channels_value_head=8, value_fc_size=256,\n",
    "                channels_policy_head=args.channels_policy_head,\n",
    "                dropout_rate=0, select_policy_from_plane=args.select_policy_from_plane,\n",
    "                kernels=kernels, se_types=se_types, use_avg_features=False, n_labels=args.n_labels,\n",
    "                use_wdl=args.use_wdl, use_plys_to_end=args.use_plys_to_end, use_mlp_wdl_ply=args.use_mlp_wdl_ply,\n",
    "                use_transformers=use_transformers, path_dropout=0.07\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = NextVit(\n",
    "    image_size = input_shape[1],\n",
    "    in_channels = input_shape[0],\n",
    "    channels_policy_head = NB_POLICY_MAP_CHANNELS,\n",
    "    stage3_repeat=1,\n",
    "    channels=256,\n",
    "    select_policy_from_plane=tc.select_policy_from_plane,\n",
    "    use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end,\n",
    "    use_mlp_wdl_ply=tc.use_mlp_wdl_ply,\n",
    "    use_transformer_heads=False, #True,\n",
    "    se_type=None, #'eca_se'\n",
    "    use_simple_transformer_blocks=False, #True\n",
    "    ) # -> 19 pool blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = AlphaZeroResnet(nb_input_channels=input_shape[0], board_height=input_shape[1], board_width=input_shape[2],\n",
    "                channels=256, act_type='relu', num_res_blocks=19,\n",
    "                channels_value_head=8, value_fc_size=256,\n",
    "                channels_policy_head=NB_POLICY_MAP_CHANNELS,\n",
    "                select_policy_from_plane=tc.select_policy_from_plane,\n",
    "                n_labels=NB_LABELS,\n",
    "                use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end, use_mlp_wdl_ply=tc.use_mlp_wdl_ply,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = VisionTransformer(get_b8_config(), img_size=8, in_channels=input_shape[0], num_classes=NB_POLICY_MAP_CHANNELS*64,\n",
    "                          use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end, use_mlp_wdl_ply=tc.use_mlp_wdl_ply,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert MXNet Symbol to Gluon Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'gluon' and symbol is not None:\n",
    "    inputs = mx.sym.var('data', dtype='float32')\n",
    "    value_out = symbol.get_internals()[main_config['value_output']+'_output']\n",
    "    policy_out = symbol.get_internals()[main_config['policy_output']+'_output']\n",
    "    sym = mx.symbol.Group([value_out, policy_out])\n",
    "    net = mx.gluon.SymbolBlock(sym, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'gluon':\n",
    "    print(net)\n",
    "elif tc.framework == 'pytorch':\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework != 'pytorch' and symbol is not None:\n",
    "    display(mx.viz.plot_network(\n",
    "        symbol,\n",
    "        shape={'data':(1, input_shape[0], input_shape[1], input_shape[2])},\n",
    "        node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}\n",
    "    ))\n",
    "elif tc.framework == 'gluon':\n",
    "    display(mx.viz.plot_network(\n",
    "        net(mx.sym.var('data'))[1],\n",
    "        shape={'data':(1, input_shape[0], input_shape[1], input_shape[2])},\n",
    "        node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet':\n",
    "    mx.viz.print_summary(\n",
    "        symbol,\n",
    "        shape={'data':(1, input_shape[0], input_shape[1], input_shape[2])},\n",
    "    )\n",
    "elif tc.framework == 'gluon':\n",
    "    mx.viz.print_summary(\n",
    "    net(mx.sym.var('data'))[1], \n",
    "    shape={'data':(1, input_shape[0], input_shape[1], input_shape[2])},\n",
    "    ) \n",
    "elif tc.framework == 'pytorch':\n",
    "    summary(model, (input_shape[0], input_shape[1], input_shape[2]), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'pytorch':\n",
    "    dummy_input = torch.Tensor(np.expand_dims(x_val[0], axis=0)).to('cpu')\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    print(flops.total())\n",
    "    from fvcore.nn import flop_count_table\n",
    "    print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights \n",
    "(only needed if no pretrained weights are used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet':\n",
    "    # create a trainable module on compute context\n",
    "    if tc.use_wdl and tc.use_plys_to_end:\n",
    "        label_names=['value_label', 'policy_label', 'wdl_label', 'plys_to_end_label']\n",
    "    else:\n",
    "        label_names=['value_label', 'policy_label']\n",
    "    \n",
    "    model = mx.mod.Module(symbol=symbol, context=ctx, label_names=label_names)\n",
    "    model.bind(for_training=True, data_shapes=[('data', (tc.batch_size, input_shape[0], input_shape[1], input_shape[2]))],\n",
    "             label_shapes=val_iter.provide_label)\n",
    "    model.init_params(mx.initializer.Xavier(rnd_type='uniform', factor_type='avg', magnitude=2.24))\n",
    "    if tc.tar_file:\n",
    "        model.load_params(tc.export_dir + \"weights/\" + tc.tar_file)\n",
    "elif tc.framework == 'gluon':    \n",
    "    # Initializing the parameters\n",
    "    for param in net.collect_params('.*gamma|.*moving_mean|.*moving_var'):\n",
    "        net.params[param].initialize(mx.initializer.Constant(1), ctx=ctx)\n",
    "    for param in net.collect_params('.*beta|.*bias'):\n",
    "        net.params[param].initialize(mx.initializer.Constant(0), ctx=ctx)\n",
    "    for param in net.collect_params('.*weight'):\n",
    "        net.params[param].initialize(mx.init.Xavier(rnd_type='uniform', factor_type='avg', magnitude=2.24), ctx=ctx)\n",
    "\n",
    "    if tc.tar_file:\n",
    "        net.collect_params().load(tc.export_dir + \"weights/\" + tc.tar_file, ctx)\n",
    "    net.hybridize()\n",
    "elif tc.framework == 'pytorch':\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                m.bias.data.fill_(0.01)\n",
    "    #model.apply(init_weights)\n",
    "    if tc.tar_file:\n",
    "        print('load model params from file:', tc.tar_file)\n",
    "        load_torch_state(model, torch.optim.SGD(model.parameters(), lr=tc.max_lr), tc.tar_file, tc.device_id)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda(torch.device(f\"cuda:{tc.device_id}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.metrics = get_metrics(tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet':\n",
    "    train_agent = TrainerAgentMXNET(model, symbol, val_iter, tc, to, use_rtpt=True)\n",
    "elif tc.framework == 'gluon':\n",
    "    train_agent = TrainerAgentGluon(net, val_data, tc, to, use_rtpt=True)\n",
    "elif tc.framework == 'pytorch':\n",
    "    train_agent = TrainerAgentPytorch(model, val_data, tc, to, use_rtpt=True, additional_loaders=additional_data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet':\n",
    "    print(model.score(val_iter, to.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(k_steps_final, value_loss_final, policy_loss_final, value_acc_sign_final, val_p_acc_final), \\\n",
    "    (k_steps_best, val_metric_values_best) = train_agent.train(cur_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the last model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = tc.export_dir + \"weights/model-%.5f-%.3f\" % (policy_loss_final, val_p_acc_final)\n",
    "\n",
    "if tc.framework == 'mxnet':\n",
    "    # the export function saves both the architecture and the weights\n",
    "    model.save_checkpoint(prefix, epoch=k_steps_final)\n",
    "elif tc.framework == 'gluon':\n",
    "    # the export function saves both the architecture and the weights\n",
    "    net.export(prefix, epoch=k_steps_final)\n",
    "    logging.info(\"Saved checkpoint to %s-%04d.params\", prefix, k_steps_final)\n",
    "elif tc.framework == 'pytorch':\n",
    "    # the export function saves both the architecture and the weights\n",
    "    save_torch_state(model, torch.optim.SGD(model.parameters(), lr=tc.max_lr), '%s-%04d.tar' % (prefix, k_steps_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print validation metrics for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_metric_values_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy best model to best-model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_best = val_metric_values_best[\"loss\"]\n",
    "val_p_acc_best = val_metric_values_best[\"policy_acc\"]\n",
    "\n",
    "model_name = \"model-%.5f-%.3f\" % (val_loss_best, val_p_acc_best)\n",
    "model_prefix = tc.export_dir + \"weights/\" + model_name\n",
    "model_arch_path = '%s-symbol.json' % model_prefix\n",
    "model_params_path = '%s-%04d.params' % (model_prefix, k_steps_best)\n",
    "model_tar_path = '%s-%04d.tar' % (model_prefix, k_steps_best)\n",
    "\n",
    "if not os.path.exists(tc.export_dir + \"best-model\"):\n",
    "    os.mkdir(tc.export_dir + \"best-model\")\n",
    "    \n",
    "best_model_prefix = tc.export_dir + \"best-model/\" + model_name\n",
    "best_model_arch_path = '%s-symbol.json' % best_model_prefix\n",
    "best_model_params_path = '%s-%04d.params' % (best_model_prefix, k_steps_best)\n",
    "best_model_tar_path = '%s-%04d.tar' % (best_model_prefix, k_steps_best)\n",
    "\n",
    "if tc.framework == 'mxnet' or tc.framework == 'gluon':\n",
    "    shutil.copy(model_arch_path, best_model_arch_path)\n",
    "    shutil.copy(model_params_path, best_model_params_path)\n",
    "elif tc.framework == 'pytorch':\n",
    "    shutil.copy(model_tar_path, best_model_tar_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the current net object form memory\n",
    "if tc.framework == 'mxnet':\n",
    "    del model\n",
    "elif tc.framework == 'gluon':\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('load current best model:', model_params_path)\n",
    "\n",
    "if tc.framework == 'mxnet' or tc.framework == 'gluon':\n",
    "    symbol = mx.sym.load(model_arch_path)\n",
    "    inputs = mx.sym.var('data', dtype='float32')\n",
    "    value_out = symbol.get_internals()[main_config['value_output']+'_output']\n",
    "    policy_out = symbol.get_internals()[main_config['policy_output']+'_output']\n",
    "    if tc.use_wdl and tc.use_plys_to_end:\n",
    "        auxiliary_out = symbol.get_internals()[main_config['auxiliary_output']+'_output']\n",
    "        wdl_out = symbol.get_internals()[main_config['wdl_output']+'_output']\n",
    "        ply_to_end_out = symbol.get_internals()[main_config['plys_to_end_output']+'_output']\n",
    "        sym = mx.symbol.Group([value_out, policy_out, auxiliary_out, wdl_out, ply_to_end_out])\n",
    "    else:\n",
    "        sym = mx.symbol.Group([value_out, policy_out])\n",
    "    net = mx.gluon.SymbolBlock(sym, inputs)\n",
    "    net.collect_params().load(model_params_path, ctx)\n",
    "elif tc.framework == 'pytorch':\n",
    "    load_torch_state(model, torch.optim.SGD(model.parameters(), lr=tc.max_lr), model_tar_path, tc.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best val_loss: %.5f with v_policy_acc: %.5f at k_steps_best %d' % (val_loss_best, val_p_acc_best, k_steps_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.use_wdl and tc.use_plys_to_end:\n",
    "    outputs = [main_config['value_output']+'_output', main_config['policy_output']+'_output',\n",
    "               main_config['auxiliary_output']+'_output',\n",
    "               main_config['wdl_output']+'_output', main_config['plys_to_end_output']+'_output']\n",
    "else:\n",
    "    outputs = [main_config['value_output']+'_output', main_config['policy_output']+'_output',]\n",
    "\n",
    "if tc.framework == 'mxnet':\n",
    "    convert_mxnet_model_to_onnx(best_model_arch_path, best_model_params_path, \n",
    "                                outputs, \n",
    "                                tuple(input_shape), tuple([1, 8, 16, 64]), True)\n",
    "elif tc.framework == 'pytorch':\n",
    "    model_prefix = \"%s-%04d\" % (model_name, k_steps_best)\n",
    "    with torch.no_grad():\n",
    "        ctx = get_context(tc.context, tc.device_id)\n",
    "        dummy_input = torch.zeros(1, input_shape[0], input_shape[1], input_shape[2]).to(ctx)\n",
    "        export_to_onnx(model, 1,\n",
    "                       dummy_input,\n",
    "                       Path(tc.export_dir) / Path(\"best-model\"), model_prefix, tc.use_wdl and tc.use_plys_to_end,\n",
    "                       True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved json, weight & onnx files of the best model to %s\" % (tc.export_dir + \"best-model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show move predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == MODE_CHESS:\n",
    "    start_board = chess.Board()\n",
    "elif mode == MODE_CRAZYHOUSE:\n",
    "    start_board = chess.variant.CrazyhouseBoard()\n",
    "else:\n",
    "    start_board = planes_to_board(x_val[idx], normalized_input=tc.normalize, mode=mode)\n",
    "board = start_board\n",
    "print(chess.COLOR_NAMES[board.turn])\n",
    "if board.uci_variant == \"crazyhouse\":\n",
    "    print(board.pockets)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(net, x, select_policy_from_plane=False):\n",
    "    \n",
    "    out = [None, None]\n",
    "    if tc.framework == 'mxnet' or tc.framework == 'gluon':\n",
    "        pred = net(mx.nd.array(np.expand_dims(x, axis=0), ctx=ctx))\n",
    "        out[0] = pred[0].asnumpy()\n",
    "        out[1] = pred[1].softmax().asnumpy()\n",
    "    elif tc.framework == 'pytorch':\n",
    "        with torch.no_grad():\n",
    "            pred = net(torch.Tensor(np.expand_dims(x, axis=0)).to(ctx))\n",
    "            out[0] = pred[0].to(torch.device(\"cpu\")).numpy()\n",
    "            out[1] = pred[1].to(torch.device(\"cpu\")).softmax(dim=1).numpy()\n",
    "    if select_policy_from_plane:\n",
    "        out[1] = out[1][:, FLAT_PLANE_IDX]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'pytorch':\n",
    "    net = model\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start_pos = board_to_planes(board, normalize=tc.normalize, mode=mode)\n",
    "pred = predict_single(net, x_start_pos, tc.select_policy_from_plane)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_to_best_move(board, yp_val[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = 5\n",
    "selected_moves, probs = policy_to_moves(board, pred[1][0])\n",
    "selected_moves[:opts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(range(opts)[::-1], probs[:opts])\n",
    "ax = plt.gca()\n",
    "ax.set_yticks(range(opts)[::-1])\n",
    "ax.set_yticklabels(selected_moves[:opts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = start_board\n",
    "board.push_uci('e2e4')\n",
    "board.push_uci('e7e5')\n",
    "board.push_uci('f1c4')\n",
    "board.push_uci('b8c6')\n",
    "board.push_uci('d1h5')\n",
    "x_scholar_atck = board_to_planes(board, normalize=tc.normalize, mode=mode)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_single(net, x_scholar_atck, tc.select_policy_from_plane)\n",
    "\n",
    "selected_moves, probs = policy_to_moves(board, pred[1][0])\n",
    "plt.barh(range(opts)[::-1], probs[:opts])\n",
    "ax = plt.gca()\n",
    "ax.set_yticks(range(opts)[::-1])\n",
    "ax.set_yticklabels(selected_moves[:opts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.push(selected_moves[0])\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn_dataset_arrays_dict = load_pgn_dataset(dataset_type='test', part_id=0,\n",
    "                                           verbose=True, normalize=True)\n",
    "s_idcs_test = pgn_dataset_arrays_dict[\"start_indices\"]\n",
    "x_test = pgn_dataset_arrays_dict[\"x\"]\n",
    "yv_test = pgn_dataset_arrays_dict[\"y_value\"]\n",
    "yp_test = pgn_dataset_arrays_dict[\"y_policy\"]\n",
    "yplys_test = pgn_dataset_arrays_dict[\"plys_to_end\"]\n",
    "pgn_datasets_test = pgn_dataset_arrays_dict[\"pgn_dataset\"]\n",
    "phase_vector_test = pgn_dataset_arrays_dict[\"phase_vector\"]\n",
    "\n",
    "test_data = get_data_loader(x_test, yv_test, yp_test, yplys_test, phase_vector_test, tc, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tc.framework == 'mxnet':\n",
    "    metrics = metrics_gluon\n",
    "\n",
    "evaluate_metrics(to.metrics, test_data, net, nb_batches=None, sparse_policy_label=tc.sparse_policy_label, ctx=ctx,\n",
    "                 phase_weights=to.phase_weights, apply_select_policy_from_plane=tc.select_policy_from_plane,\n",
    "                 use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show result on mate-in-one problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn_dataset_arrays_dict = load_pgn_dataset(dataset_type='mate_in_one', part_id=0,\n",
    "                                           verbose=True, normalize=tc.normalize)\n",
    "\n",
    "s_idcs_mate = pgn_dataset_arrays_dict[\"start_indices\"]\n",
    "x_mate = pgn_dataset_arrays_dict[\"x\"]\n",
    "yv_mate = pgn_dataset_arrays_dict[\"y_value\"]\n",
    "yp_mate = pgn_dataset_arrays_dict[\"y_policy\"]\n",
    "yplys_mate = pgn_dataset_arrays_dict[\"plys_to_end\"]\n",
    "pgn_dataset_mate = pgn_dataset_arrays_dict[\"pgn_dataset\"]\n",
    "phase_vector_mate = pgn_dataset_arrays_dict[\"phase_vector\"]\n",
    "\n",
    "yplys_mate = np.ones(len(yv_mate))\n",
    "mate_data = get_data_loader(x_mate, yv_mate, yp_mate, yplys_mate, phase_vector_mate, tc, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mate In One Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(to.metrics, mate_data, net, nb_batches=None, sparse_policy_label=tc.sparse_policy_label, ctx=ctx,\n",
    "                 phase_weights=to.phase_weights, apply_select_policy_from_plane=tc.select_policy_from_plane,\n",
    "                 use_wdl=tc.use_wdl, use_plys_to_end=tc.use_plys_to_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some example mate problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pos(net, x_mate, yp_mate, verbose=False, select_policy_from_plane=False):\n",
    "    \n",
    "    board = planes_to_board(x_mate, normalized_input=tc.normalize, mode=mode)\n",
    "    if verbose is True:\n",
    "        print(\"{0}'s turn\".format(chess.COLOR_NAMES[board.turn]))\n",
    "        if board.uci_variant == \"crazyhouse\":\n",
    "            print(\"black/white {0}\".format(board.pockets))\n",
    "    pred = predict_single(net, x_mate, select_policy_from_plane=select_policy_from_plane)\n",
    "    \n",
    "    true_move = policy_to_move(yp_mate, mirror_policy=board.turn==chess.BLACK)\n",
    "    \n",
    "    opts = 5\n",
    "    pred_moves, probs = policy_to_moves(board, pred[1][0])\n",
    "    pred_moves = pred_moves[:opts]\n",
    "    \n",
    "    legal_move_cnt = board.legal_moves.count()\n",
    "    mate_move_cnt = str(board.legal_moves).count('#')\n",
    "    \n",
    "    is_mate_5_top = False\n",
    "    \n",
    "    for pred_move in pred_moves:\n",
    "        board_5_top = deepcopy(board)\n",
    "        board_5_top.push(pred_move)\n",
    "        if board_5_top.is_checkmate() is True:\n",
    "            is_mate_5_top = True\n",
    "            break\n",
    "    \n",
    "    board.push(pred_moves[0])\n",
    "    \n",
    "    is_checkmate = False\n",
    "    if board.is_checkmate() is True:\n",
    "        is_checkmate = True\n",
    "        \n",
    "    filtered_pred = sorted(pred[1][0], reverse=True)\n",
    "    \n",
    "    if verbose is True:\n",
    "        plt.barh(range(opts)[::-1], filtered_pred[:opts])\n",
    "        ax = plt.gca()\n",
    "        ax.set_yticks(range(opts)[::-1])\n",
    "        ax.set_yticklabels(pred_moves)\n",
    "        plt.title('True Move:' + str(true_move) +\n",
    "                 '\\nEval:' + str(pred[0][0]))\n",
    "        plt.show()\n",
    "    \n",
    "    return pred, pred_moves, true_move, board, is_checkmate, is_mate_5_top, legal_move_cnt, mate_move_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pos = len(x_mate)\n",
    "mates_found = []\n",
    "mates_5_top_found = []\n",
    "legal_mv_cnts = []\n",
    "mate_mv_cnts = []\n",
    "\n",
    "for i in range(nb_pos):\n",
    "    pred, pred_moves, true_move, board, is_mate, is_mate_5_top, legal_mv_cnt, mate_mv_cnt= eval_pos(net, x_mate[i], yp_mate[i], select_policy_from_plane=tc.select_policy_from_plane)\n",
    "    mates_found.append(is_mate)\n",
    "    legal_mv_cnts.append(legal_mv_cnt)\n",
    "    mate_mv_cnts.append(mate_mv_cnt)\n",
    "    mates_5_top_found.append(is_mate_5_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mate_mv_cnts).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(legal_mv_cnts).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Guessing Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mate_mv_cnts).mean() / np.array(legal_mv_cnts).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciton Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mate_in_one_acc:', sum(mates_found) / nb_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mates_5_top_found) / nb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn_dataset_mate.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = np.array(pgn_dataset_mate['metadata'])\n",
    "metadata[0, :]\n",
    "metadata[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_mate = metadata[1:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(np_string):\n",
    "    string = str(site_mate[i]).replace(\"b'\", \"\")\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    string = string.replace('\"', '')\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.svg\n",
    "from IPython.display import SVG, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the result of the first 17 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17):\n",
    "    print(clean_string(site_mate[i]))\n",
    "    pred, pred_moves, true_move, board, is_checkmate, is_mate_5_top, legal_move_cnt, mate_move_cnt = eval_pos(net, x_mate[i], yp_mate[i], verbose=True, select_policy_from_plane=tc.select_policy_from_plane)\n",
    "    pred_move = pred_moves[0]\n",
    "    pred_arrow = chess.svg.Arrow(pred_move.from_square, pred_move.to_square)\n",
    "    SVG(data=chess.svg.board(board=board, arrows=[pred_arrow], size=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show examples where it failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mate_missed = 0\n",
    "for i in range(1000):\n",
    "    pred, pred_moves, true_move, board, is_checkmate, is_mate_5_top, legal_move_cnt, mate_move_cnt = eval_pos(net, x_mate[i], yp_mate[i], verbose=False, select_policy_from_plane=tc.select_policy_from_plane)\n",
    "    if is_mate_5_top is False:\n",
    "        mate_missed += 1\n",
    "        print(clean_string(site_mate[i]))\n",
    "        pred, pred_moves, true_move, board, is_checkmate, is_mate_5_top, legal_move_cnt, mate_move_cnt = eval_pos(net, x_mate[i], yp_mate[i], verbose=True, select_policy_from_plane=tc.select_policy_from_plane)\n",
    "        pred_move = pred_moves[0]\n",
    "        pred_arrow = chess.svg.Arrow(pred_move.from_square, pred_move.to_square)\n",
    "        SVG(data=chess.svg.board(board=board, arrows=[pred_arrow], size=400))\n",
    "    if mate_missed == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}